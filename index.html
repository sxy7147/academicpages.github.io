<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yan Shen</title>
  
  <meta name="author" content="Yan Shen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⭐️</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yan Shen (Yan Zhao)</name>
              </p>
              <!-- <p>I am a second-year PhD Candidate in <a href="https://cfcs.pku.edu.cn/english/"> Center on Frontiers of Computing Studies
                (CFCS)</a> at <a href="https://english.pku.edu.cn/">Peking University</a>, advised by Professor Hao Dong. Previously, I got my Bachelor degree from Yuanpei College, Peking
              University, in 2022. -->
              <p>I am a second-year PhD Candidate in <a href="https://cfcs.pku.edu.cn/english/"> Center on Frontiers of Computing Studies (CFCS)</a> at <a href="https://english.pku.edu.cn/">Peking University</a>, advised by Professor <a href="https://zsdonghao.github.io/">Hao Dong</a>. Previously, I got my Bachelor degree from Yuanpei College, Peking University, in 2022.
              </p>
              <p>
                  My research interests include Computer Vision and Robotics.
              </p>
              <p>
                I used to go by the name Yan Zhao until the summer of 2023, when I changed it to Yan Shen.
              </p>
              <p>
                Email: yan790 [at] pku.edu.cn
              </p>
              <p style="text-align:center">
                <a href="mailto:yan790@pku.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=iIs4TDMAAAAJ">Google Scholar</a> 
                <!-- <a href="https://github.com/sxy7147">Github</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/selfie.JPG"><img style="width:80%;max-width:100%" alt="profile photo" src="images/selfie.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>&nbsp&nbsp&nbsp&nbsp&nbsp (* denotes equal contribution)
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;"><tbody>


          <tr onmouseout="clutter_stop()" onmouseover="clutter_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one" style="padding-top: 50px;">
                <div class="two" id='clutter_image'>
                  <img src='images/clutter.png' width="187" style="vertical-align:middle;">
                </div>
                <img src='images/clutter.png' width="187" style="vertical-align:middle;">
              </div>
              <script type="text/javascript">
                function clutter_start() {
                  document.getElementById('clutter_image').style.opacity = "1";
                }

                function clutter_stop() {
                  document.getElementById('clutter_image').style.opacity = "0";
                }
                clutter_stop()
              </script>
            </td>

            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://sxy7147.github.io">
                <papertitle>Broadcasting Support Relations Recursively from Local Dynamics for Object Retrieval in Clutters
                </papertitle>
              </a>
              <br>
              Yitong Li*,
              <a href="https://warshallrho.github.io">Ruihai Wu</a>*,
              Haoran Lu,
              <a href="https://tritiumr.github.io/">Chuanruo Ning</a>,
              <strong>Yan Shen</strong>,
              <a href="https://www.robots.ox.ac.uk/~guanqi/">Guanqi Zhan</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>RSS 2024</em>
              <br>
              <a href="https://sxy7147.github.io">project page (coming soon)</a>
              /
              <a href="https://arxiv.org/abs/2406.02283">paper</a>
              <!--              paper (coming soon)-->
              /
              <!--              <a href="https://github.com/luhr2003/UniGarmentManip/">code</a>-->
              code (coming soon)
              /
              video (coming soon)
              <!--              <a href="https://www.youtube.com/watch?v=N5NYt-XJDOs">video</a>-->
              <p></p>
              <!-- <p>In this paper, we study retrieving objects in complicated clutters via a novel method of recursively broadcasting -->
                <!-- the accurate local dynamics to build a support relation graph of the whole scene, which largely reduces the
                complexity of the support relation inference and improves the accuracy. </p> -->
            </td>
            </tr>


          <tr onmouseout="manipllm_stop()" onmouseover="manipllm_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one" style="padding-top: 50px;">
                <div class="two" id='ManipLLM_image'>
                  <img src='images/ManipLLM.jpg' width="187" style="vertical-align:middle;">
                </div>
                <img src='images/ManipLLM.jpg' width="187" style="vertical-align:middle;">
              </div>
              <script type="text/javascript">
                function clutter_start() {
                  document.getElementById('ManipLLM').style.opacity = "1";
                }

                function clutter_stop() {
                  document.getElementById('ManipLLM').style.opacity = "0";
                }
                clutter_stop()
              </script>
            </td>
          
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/manipllm">
                <papertitle>ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation
                </papertitle>
              </a>
              <br>
              <a href="https://clorislili.github.io/clorisLi/">Xiaoqi Li</a>*,
              Mingxu Zhang,
              <a href="https://gengyiran.github.io">Yiran Geng</a>,
              <a href="https://geng-haoran.github.io/">Haoran Geng</a>,
              <a href="https://lyx0501.github.io/">Yuxing Long</a>,
              <strong>Yan Shen</strong>,
              <a href="https://zrrskywalker.github.io/">Renrui Zhang</a>,
              <a href="https://liujiaming1996.github.io/">Jiaming Liu</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>CVPR 2024</em>
              <br>
              <a href="https://sites.google.com/view/manipllm">project page</a>
              /
              <a href="https://arxiv.org/pdf/2312.16217">paper</a>
              /
              <!-- <a href="https://github.com/luhr2003/UniGarmentManip/">code</a> -->
              code (coming soon)
              <p></p>
            </td>
          </tr>
               

          <tr onmouseout="env_stop()" onmouseover="env_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one" style="padding-top: 50px;">
                <div class="two" id='env_image'>
                  <img src='images/env.jpg' width="187" style="vertical-align:middle;">
                </div>
                <img src='images/env.jpg' width="187" style="vertical-align:middle;">
              </div>
              <script type="text/javascript">
                function env_start() {
                  document.getElementById('env_image').style.opacity = "1";
                }

                function env_stop() {
                  document.getElementById('env_image').style.opacity = "0";
                }
                env_stop()
              </script>
            </td>
          
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://neurips.cc/virtual/2023/poster/71652">
                <papertitle>Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions
                </papertitle>
              </a>
              <br>
              <a href="https://warshallrho.github.io">Ruihai Wu</a>*,
              <a href="https://chengkaiacademycity.github.io/">Kai Cheng</a>*,
              <strong>Yan Shen</strong>,
              <a href="https://tritiumr.github.io/">Chuanruo Ning</a>,
              <a href="https://www.robots.ox.ac.uk/~guanqi/">Guanqi Zhan</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>NeurIPS 2023</em>
              <br>
              <a href="https://chengkaiacademycity.github.io/EnvAwareAfford/">project page</a>
              /
              <a href="https://neurips.cc/virtual/2023/poster/71652">paper</a>
              /
              <a href="https://github.com/chengkaiAcademyCity/EnvAwareAfford">code</a>
              /
              <a href="https://youtu.be/r88hWcRmQYM">video</a>
              <p></p>
              <!-- <p>We explore the task of manipulating articulated objects within environment constraints and formulate the task of
                environment-aware affordance learning for manipulating 3D articulated objects, incorporating object-centric
                per-point priors and environment constraints. </p> -->
            </td>
          </tr>

          

          <tr onmouseout="assembly_stop()" onmouseover="assembly_start()">
            <td style="padding:20px; width:25%; vertical-align:middle;">
              <div class="one">
                <div class="two" id='assembly_image'>
                  <img src='images/assembly.gif' width="187">
                </div>
                <img src='images/assembly.gif' width="187">
              </div>
              <script type="text/javascript">
                function assembly_start() {
                  document.getElementById('assembly_image').style.opacity = "1";
                }

                function assembly_stop() {
                  document.getElementById('assembly_image').style.opacity = "0";
                }
                assembly_stop()
              </script>
            </td>
          
            <td style="padding:20px; width:75%; vertical-align:middle;">
              <a href="https://crtie.github.io/SE-3-part-assembly/">
                <papertitle>Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly
                </papertitle>
              </a>
              <br>
              <a href="https://warshallrho.github.io">Ruihai Wu</a>*,
              <a href="https://crtie.github.io/">Chenrui Tie</a>*,
              Yushi Du*,
              <strong>Yan Shen</strong>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>ICCV 2023</em>
              <br>
              <a href="https://crtie.github.io/SE-3-part-assembly/">project page</a>
              /
              <a href="https://arxiv.org/abs/2309.06810">paper</a>
              /
              <a href="https://github.com/crtie/Leveraging-SE-3-Equivariance-for-Learning-3D-Geometric-Shape-Assembly">code</a>
              /
              <a href="https://www.youtube.com/watch?v=pEtIAal-xgQ">video</a>
              <p></p>
              <!-- <p>We study geometric shape assembly by leveraging SE(3) Equivariance, which disentangles poses and shapes of fractured parts. </p> -->
            </td>
          </tr>


          <tr onmouseout="dualafford_stop()" onmouseover="dualafford_start()">
            <td style="padding:20px; width:25%; vertical-align:middle;">
              <div class="one" style="padding-top: 50px;">
                <div class="two" id='dualafford_image'>
                  <img src='images/2022-dualafford.gif' width="187" height="110">
                </div>
                <img src='images/2022-dualafford.gif' width="187" height="110">
              </div>
              <script type="text/javascript">
                function dualafford_start() {
                  document.getElementById('dualafford_image').style.opacity = "1";
                }

                function dualafford_stop() {
                  document.getElementById('dualafford_image').style.opacity = "0";
                }
                dualafford_stop()
              </script>
            </td>
            <td style="padding:20px; width:75%; vertical-align:middle;">
              <a href="https://hyperplane-lab.github.io/DualAfford/">
                <papertitle>DualAfford: Learning Collaborative Visual Affordance for Dual-gripper Object Manipulation
                </papertitle>
              </a>
              <br>
              <strong>Yan Zhao</strong>*,
              <a href="https://warshallrho.github.io">Ruihai Wu</a>*,
              Zhehuan Chen,
              <a href="https://www.linkedin.com/in/yourong-zhang-2b1aab23a/">Yourong Zhang</a>,
              <a href="https://fqnchina.github.io/">Qingnan Fan</a>,
              <a href="https://kaichun-mo.github.io/">Kaichun Mo</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>ICLR 2023</em>
              <br>
              <a href="https://hyperplane-lab.github.io/DualAfford/">project page</a>
              /
              <a href="https://arxiv.org/pdf/2207.01971.pdf">paper</a>
              /
              <a href="https://github.com/sxy7147/DualAfford">code</a>
              /
              <a href="https://youtu.be/J0UUE2FvOCE">video</a>
              <p></p>
              <!-- <p>We study collaborative affordance for dual-gripper manipulation. The core is to reduce the quadratic problem for two grippers into two disentangled yet interconnected subtasks. </p> -->
            </td>
          </tr>


          <tr onmouseout="dpart_stop()" onmouseover="dpart_start()">
            <td style="padding:20px; width:25%; vertical-align:middle;">
              <div class="one"  style="padding-top: 50px;">
                <div class="two" id='dpart_image'>
                  <img src='images/dpart.png' width="187"></div>
                <img src='images/dpart.png' width="187">
              </div>
              <script type="text/javascript">
                function dpart_start() {
                  document.getElementById('dpart_image').style.opacity = "1";
                }

                function dpart_stop() {
                  document.getElementById('dpart_image').style.opacity = "0";
                }
                dpart_stop()
              </script>
            </td>

            <td style="padding:20px; width:75%; vertical-align:middle;">
              <a href="https://hyperplane-lab.github.io/DPart/">
              <papertitle>Learning Part Motion of Articulated Objects Using Spatially Continuous Neural Implicit Representations 
              </papertitle>
              </a>
              <br>
              Yushi Du*,
              <a href="https://warshallrho.github.io">Ruihai Wu</a>*,
              <strong>Yan Shen</strong>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>BMVC 2023</em>
              <br>
              <a href="https://hyperplane-lab.github.io/DPart/">project page</a>
              /
              <a href="https://arxiv.org/abs/2311.12407">paper</a>
              /
              <a href="https://github.com/Yushi-Du/PartMotion">code</a>
              /
              <!-- <a href="https://www.youtube.com/watch?v=pEtIAal-xgQ">video</a>
              <p></p> -->
            </td>
          </tr>


          <tr onmouseout="vatmart_stop()" onmouseover="vatmart_start()">
            <td style="padding:20px; width:25%; vertical-align:middle;">
              <div class="one" style="padding-top: 30px;">
                <div class="two" id='vatmart_image'>
                  <img src='images/vat_mart.gif' width="187"></div>
                <img src='images/vat_mart.gif' width="187">
              </div>
              <script type="text/javascript">
                function vatmart_start() {
                  document.getElementById('vatmart_image').style.opacity = "1";
                }

                function vatmart_stop() {
                  document.getElementById('vatmart_image').style.opacity = "0";
                }
                vatmart_stop()
              </script>
            </td>
            <td style="padding:20px; width:75%; vertical-align:middle;">
              <a href="https://hyperplane-lab.github.io/vat-mart/">
              <papertitle>VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects
              </papertitle>
              </a>
              <br>
              <a href="https://warshallrho.github.io">Ruihai Wu</a>*,
              <strong>Yan Zhao</strong>*,
              <a href="https://kaichun-mo.github.io/">Kaichun Mo</a>*,
              <a href="https://guozz.cn/">Zizheng Guo</a>,
              <a href="https://github.com/wangyian-me">Yian Wang</a>,
              <a href="https://tianhaowuhz.github.io/">Tianhao Wu</a>,
              <a href="https://fqnchina.github.io/">Qingnan Fan</a>,
              <a href="https://xuelin-chen.github.io/">Xuelin Chen</a>,
              <a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>ICLR 2022</em>
              <br>
              <a href="https://hyperplane-lab.github.io/vat-mart/">project page</a>
              /
              <a href="https://arxiv.org/pdf/2106.14440.pdf">paper</a>
              /
              <a href="https://github.com/warshallrho/VAT-Mart">code</a>
              /
              <a href="https://www.youtube.com/watch?v=HjhsLKf1eQY">video</a>
              <p></p>
              <!-- <p>We study dense geometry-aware, interaction-aware, and task-aware visual action affordance and trajectory proposals for manipulating articulated objects. </p> -->
            </td>
          </tr>

        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              Reviewer: Neurips 2024, ECCV 2024, WACV 2024, CVPR <em>2023</em> (Outstanding Reviewer)
              <br>
            </td>
          </tr>
        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching Assistant</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
            <td width="75%" valign="center">
                <a href="https://hughw19.github.io/Intro2CV_23Spring/">Introduction to Computer Vision</a>, &nbsp&nbsp&nbsp <em>Spring, 2023</em>
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Honors and Awards</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr style="line-height: 2;">
            <td width="75%" valign="center">
              Outstanding Reviewer, &nbsp&nbsp&nbsp  <em>CVPR, 2023</em>
              <br>
              First Prize in Undergraduate Scientific Research (5 in Yuanpei College), &nbsp&nbsp&nbsp <em>Peking University, 2022</em>
              <br>
              Outstanding Undergraduate Thesis, &nbsp&nbsp&nbsp <em>CFCS, Peking University, 2022</em>
              <br>
              Freshmen Scholarship, &nbsp&nbsp&nbsp  <em>Peking University, 2018</em>
              <br>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
                  <div style="float:right;">
                    Website template comes from <a href="https://jonbarron.info/">Jon Barron</a><br>
                      <!-- Last update: April, 2023 -->
                  </div>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
